{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rf = pd.read_csv(r'data.csv')\n",
    "\n",
    "print(df_rf.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [column for column in df_rf]\n",
    "columns.insert(0, 'Method')\n",
    "    \n",
    "results_original_data = pd.DataFrame(columns = columns)\n",
    "results_scaled_data = pd.DataFrame(columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min Max Scaling\n",
    "\n",
    "##### X' = (X - min(X)) / (max(X) - min(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset using Min Max scaling method\n",
    "def min_max_scaling(df):\n",
    "    scaled_df = (df - df.min())/(df.max() - df.min())\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates artificial missingness in the dataset\n",
    "def miss_values(df):\n",
    "    df_mv = df.copy()\n",
    "    \n",
    "    # randomly selecting 50% indices from the length of dataframe\n",
    "    rand_miss_instances = rand.sample(range(0, len(df_mv) - 1), math.floor(len(df_mv)/2))\n",
    "    \n",
    "    # number of values to be made NaN for every feature\n",
    "    rand_miss_values_count = math.floor(len(rand_miss_instances) / 2)\n",
    "    \n",
    "    # replacing the identified feature value as NaN\n",
    "    for feature in df_mv:\n",
    "        rand_miss_values = rand.sample(rand_miss_instances, rand_miss_values_count)\n",
    "        for value in rand_miss_values:\n",
    "            df_mv.loc[value, feature] = np.nan\n",
    "            \n",
    "    return df_mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_mse(original_values, imputed_values):\n",
    "    mse = 0\n",
    "    total_imputed = 0\n",
    "    \n",
    "    # distance can be calculated only if both the arrays are of same length\n",
    "    if(len(original_values) == len(imputed_values)):\n",
    "        for i in range(len(original_values)):\n",
    "            \n",
    "            # discarding the identical values in original and imputed df since the distance between them is zero\n",
    "            if(original_values[i] != imputed_values[i]):\n",
    "                total_imputed += 1\n",
    "                \n",
    "                # squared error is summation of square of the difference between original and imputed value\n",
    "                mse += (original_values[i] - imputed_values[i]) ** 2\n",
    "        \n",
    "        \n",
    "        # mean squared error is average of squared error\n",
    "        mse = mse / len(original_values)\n",
    "    \n",
    "    else:\n",
    "        print(\"Original values length: {}\".format(len(original_values)))\n",
    "        print(\"Imputed values length: {}\".format(len(imputed_values)))\n",
    "        return np.nan\n",
    "        \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df_rf, df_imputed):\n",
    "    result = []\n",
    "    for feature in df_rf:\n",
    "        accuracy = accuracy_mse(df_rf[feature], df_imputed[feature])\n",
    "        print(\"Original values length: {}\".format(len(df_rf[feature])))\n",
    "        print(\"Imputed values length: {}\".format(len(df_imputed[feature])))\n",
    "        print(\"{} is {}\".format(feature, accuracy))\n",
    "        result.append(accuracy)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(dp1, dp2):\n",
    "    # dp1 is row with missing values\n",
    "    # dp2 is row to be matched with dp1 for similarity using distance\n",
    "    sum = 0\n",
    "    \n",
    "    # calculating summation of the square of difference between two points dp1[i] and dp2[j]\n",
    "    for i in range(len(dp1)):\n",
    "        \n",
    "        # discarding nan values from dp1 and finding neighbours for dp1 with only available features in dp1\n",
    "        if not math.isnan(dp1[i]):\n",
    "            sum = sum + ((dp1[i] - dp2[i]) ** 2)\n",
    "           \n",
    "    # euclidean distance is the square root of the above summation\n",
    "    return math.sqrt(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours(missed_row, df, k, incomplete_instances):\n",
    "    min = k\n",
    "    current_max = {'index':np.nan, 'max':0}\n",
    "    nearest_neighbours = []\n",
    "    # iterate through the df\n",
    "    for index, current_row in df.iterrows():\n",
    "        \n",
    "        # considering only the complete instances\n",
    "        # check if the df's current row does not contain any null values and \n",
    "        # also current_row is from the 50% of the complete instances\n",
    "        if not current_row.isna().sum() and index not in incomplete_instances:\n",
    "            # if any row found, calculate the distance between the missed row and the current row\n",
    "            distance = euclidean_dist(missed_row, current_row)\n",
    "            \n",
    "            # insert the first 3 directly into the nearest neighbours list, current_max contains the \n",
    "            # max distance out of the current 3 items in the nearest neighbours list\n",
    "            if min > 0:\n",
    "                nearest_neighbours.append({'index':index, 'nn':current_row, 'distance':distance})\n",
    "                min = min - 1\n",
    "                if distance >= current_max['max']:\n",
    "                    current_max['max'] = distance\n",
    "                    current_max['index'] = index\n",
    "            \n",
    "            # if there are already 3 items in the nearest neighbours list, check if the recently\n",
    "            # calculated distance is less than the current max value, if yes, remove the current max item\n",
    "            # from the list and add the new neighbour to the list and update the current max accordingly\n",
    "            else:\n",
    "                if distance < current_max['max']:\n",
    "                    nearest_neighbours = [neighbour for neighbour in nearest_neighbours if neighbour['index'] != current_max['index']]\n",
    "                    nearest_neighbours.append({'index':index, 'nn':current_row, 'distance':distance})\n",
    "                    \n",
    "                    max_neighbour = max(nearest_neighbours, key=lambda x:x['distance'])\n",
    "                    current_max['max'] = max_neighbour['distance']\n",
    "                    current_max['index'] = max_neighbour['index']\n",
    "                    \n",
    "    print(\"Nearest Neighbours for {}\".format(missed_row))\n",
    "    print(nearest_neighbours)\n",
    "                    \n",
    "    return nearest_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_mean(df):\n",
    "    #df_imputed = df.copy()\n",
    "    \n",
    "    mean_dict = {}\n",
    "    \n",
    "    complete_instances = df.copy().dropna()\n",
    "    \n",
    "    for feature in complete_instances:\n",
    "        mean_dict[feature] = round(complete_instances[feature].mean(), 3)\n",
    "    \n",
    "    print(\"Mean of complete instances\")\n",
    "    print(mean_dict)\n",
    "    \n",
    "    for feature in df:\n",
    "        #print(\"Mean of feature {} is {}\".format(feature, round(df_imputed[feature].mean(), 1)))\n",
    "        # replace all the nan values with the mean value of the feature\n",
    "        df[feature] = df[feature].replace(np.nan, mean_dict[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_nn(df, k, weighted):\n",
    "    incomplete_instances = []   #array to keep track of rows that has missing cells imputed\n",
    "    \n",
    "    # Imputing is done row wise - a row with nan values is selected and all the \n",
    "    # nan values are imputed according to the nearest neighbours\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # considering rows that contains nan values for imputations\n",
    "        if row.isna().sum():\n",
    "            incomplete_instances.append(index)\n",
    "            print(\"\\nRow {} has missing values\".format(index))\n",
    "            # get the nearest neighbours\n",
    "            nn_df_dict = get_nearest_neighbours(row, df, k, incomplete_instances)\n",
    "            \n",
    "            # sorting the list of dictionary of nearest neighbours in descending order\n",
    "            # with respect to distance incase of weighted knn\n",
    "            sorted(nn_df_dict, key = lambda nn: nn['distance'], reverse = True)\n",
    "            nn_df = [nn['nn'] for nn in nn_df_dict]\n",
    "            \n",
    "            # iterate through the columns to find if its nan\n",
    "            for key in row.keys():\n",
    "                if(math.isnan(row[key])):\n",
    "                    mean_nn = 0\n",
    "                    # if nan found, get the key value and find the mean / weighted mean of the key values \n",
    "                    # from the received nearest neighbours depending on weighted parameter\n",
    "                    \n",
    "                    # if not weighted knn\n",
    "                    if not weighted:\n",
    "                        for nn in nn_df:\n",
    "                            mean_nn = mean_nn + nn[key]\n",
    "                        \n",
    "                        mean_nn = mean_nn / k\n",
    "                        \n",
    "                    # if weighted knn, weight assigned to every neighbour is 1/distance\n",
    "                    else:\n",
    "                        # calculating weights of neighbour with respect to distance\n",
    "                        distances = [nn['distance'] for nn in nn_df_dict]\n",
    "                        weights = []\n",
    "                        \n",
    "                        # if any of the neighbours has distance 0, then total weightage is given to those\n",
    "                        # neighbours and values are picked from those neighbours only with full weightage\n",
    "                        # if distance is 0, the two points are identical\n",
    "                        if 0 in distances:\n",
    "                            print(\"One of the neighbour has distance 0\")\n",
    "                            for i in range(len(distances)):\n",
    "                                if distances[i] == 0:\n",
    "                                    weights.append(1)\n",
    "                                else:\n",
    "                                    weights.append(0)\n",
    "                        \n",
    "                        # if not, then weight is 1/distance for the neighbour\n",
    "                        else:\n",
    "                            for i in range(len(distances)):\n",
    "                                weights.append(1/distances[i])\n",
    "                            \n",
    "                        i = 0\n",
    "                        for nn in nn_df_dict:\n",
    "                            mean_nn = mean_nn + (nn['nn'][key] * weights[i])\n",
    "                            i = i + 1\n",
    "                            \n",
    "                        mean_nn = mean_nn / sum(weights)\n",
    "                    \n",
    "                    \n",
    "                    # replace the nan value with the mean value from above step\n",
    "                    print(\"Before imputation {}[{}] is {}\".format(key, index, row[key]))\n",
    "                    row[key] = mean_nn\n",
    "                    print(\"After imputation {}[{}] is {}\".format(key, index, row[key]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df - dataframe for operation\n",
    "# method - possible values - mean, knn\n",
    "# k - number of nearest neighbours for kKNN, default is 1\n",
    "# weighted - weighted KNN if value is true, default is false\n",
    "\n",
    "def imputation(df, method, k=1, weighted=False):\n",
    "    if method == 'mean':\n",
    "        imputation_mean(df)\n",
    "    \n",
    "    elif method == 'knn':\n",
    "        imputation_nn(df, k, weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf_scaled = min_max_scaling(df_rf)\n",
    "\n",
    "df_rf_scaled.to_csv('data_scaled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mv = miss_values(df_rf)\n",
    "\n",
    "df_mv_s = miss_values(df_rf_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mv.to_csv('dataset_mv.csv', index=False)\n",
    "\n",
    "df_mv_s.to_csv('dataset_mv_s.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating individual dataframes for every imputation method for simplicity\n",
    "df_mv_mean = pd.read_csv('dataset_mv.csv')\n",
    "df_mv_1nn = pd.read_csv('dataset_mv.csv')\n",
    "df_mv_3nn = pd.read_csv('dataset_mv.csv')\n",
    "df_mv_5nn = pd.read_csv('dataset_mv.csv')\n",
    "df_mv_w_1nn = pd.read_csv('dataset_mv.csv')\n",
    "df_mv_w_3nn = pd.read_csv('dataset_mv.csv')\n",
    "df_mv_w_5nn = pd.read_csv('dataset_mv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mv_s_mean = pd.read_csv('dataset_mv_s.csv')\n",
    "df_mv_s_1nn = pd.read_csv('dataset_mv_s.csv')\n",
    "df_mv_s_3nn = pd.read_csv('dataset_mv_s.csv')\n",
    "df_mv_s_5nn = pd.read_csv('dataset_mv_s.csv')\n",
    "df_mv_w_s_1nn = pd.read_csv('dataset_mv_s.csv')\n",
    "df_mv_w_s_3nn = pd.read_csv('dataset_mv_s.csv')\n",
    "df_mv_w_s_5nn = pd.read_csv('dataset_mv_s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_mean, method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_1nn, method='knn', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_3nn, method='knn', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_5nn, method='knn', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_w_1nn, method='knn', k=1, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_w_3nn, method='knn', k=3, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_w_5nn, method='knn', k=5, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation(df_mv_s_mean, method='mean')\n",
    "imputation(df_mv_s_1nn, method='knn', k=1)\n",
    "imputation(df_mv_s_3nn, method='knn', k=3)\n",
    "imputation(df_mv_s_5nn, method='knn', k=5)\n",
    "imputation(df_mv_w_s_1nn, method='knn', k=1, weighted=True)\n",
    "imputation(df_mv_w_s_3nn, method='knn', k=3, weighted=True)\n",
    "imputation(df_mv_w_s_5nn, method='knn', k=5, weighted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_original_data.loc[0] = ['Mean'] + calculate_accuracy(df_rf, df_mv_mean)\n",
    "results_original_data.loc[1] = ['1NN'] + calculate_accuracy(df_rf, df_mv_1nn)\n",
    "results_original_data.loc[2] = ['3NN'] + calculate_accuracy(df_rf, df_mv_3nn)\n",
    "results_original_data.loc[3] = ['5NN'] + calculate_accuracy(df_rf, df_mv_5nn)\n",
    "results_original_data.loc[5] = ['1WNN'] + calculate_accuracy(df_rf, df_mv_w_1nn)\n",
    "results_original_data.loc[6] = ['3WNN'] + calculate_accuracy(df_rf, df_mv_w_3nn)\n",
    "results_original_data.loc[7] = ['5WNN'] + calculate_accuracy(df_rf, df_mv_w_5nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scaled_data.loc[0] = ['Mean'] + calculate_accuracy(df_rf_scaled, df_mv_s_mean)\n",
    "results_scaled_data.loc[1] = ['1NN'] + calculate_accuracy(df_rf_scaled, df_mv_s_1nn)\n",
    "results_scaled_data.loc[2] = ['3NN'] + calculate_accuracy(df_rf_scaled, df_mv_s_3nn)\n",
    "results_scaled_data.loc[3] = ['5NN'] + calculate_accuracy(df_rf_scaled, df_mv_s_5nn)\n",
    "results_scaled_data.loc[5] = ['1WNN'] + calculate_accuracy(df_rf_scaled, df_mv_w_s_1nn)\n",
    "results_scaled_data.loc[6] = ['3WNN'] + calculate_accuracy(df_rf_scaled, df_mv_w_s_3nn)\n",
    "results_scaled_data.loc[7] = ['5WNN'] + calculate_accuracy(df_rf_scaled, df_mv_w_s_5nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_original_data.to_csv('results_original_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_scaled_data.to_csv('results_scaled_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
